# S3DPN



## Abstract
The Railway Point Machine (RPM) serves as a critical component in railway systems, yet its high reliability results in extreme fault data scarcity, posing significant challenges for data-driven RPM fault diagnosis. Although existing small-sample learning methods for RPM have demonstrated promising potential, they still face three fundamental limitations: (1) over-reliance on limited RPM fault data while neglecting available external mechanical fault datasets, (2) insufficient prototype representation caused by inadequate feature learning from scarce samples, and (3) exacerbated overfitting tendencies due to complex optimization procedures under small-sample conditions. To address these limitations, we propose the Self-Supervised and Self-Distilled Prototypical Network (S3DPN), which systematically resolves these issues through: (1) self-supervised pretraining with external mechanical fault data to enrich feature representations, (2) prototype-based fine-tuning with auxiliary self-supervised losses to enhance inter-class separability, and (3) a simplified self-distillation strategy that maintains computational efficiency while effectively mitigating overfitting. Extensive experiments on real-world ZDJ9-type RPM datasets demonstrate that S3DPN consistently outperforms state-of-the-art methods across various small-sample scenarios. Our code will be released at https://github.com/rookiemyf/S3DPN.


## Sensitivity Analysis

In the experiments on the CWRU dataset, Fig.~\ref{CWRU} reports the grid‐search landscape, clearly illustrating the interplay between the self‐supervised weight $\alpha$ and the distillation weight $\beta$. In the $N$‐way 1‐shot regime, the model accuracy deteriorates once $\alpha$ exceeds 0.3, whereas keeping $\alpha$ within the narrow band 0.1–0.2 consistently yields superior stability.  This observation indicates that a modest self‐supervised contribution is pivotal for robust feature learning under extreme data scarcity. Conversely, in the $N$‐way 5‐shot setting, the best performance emerges when $\beta$ lies in the range 0.4–1.0 and $\alpha$ is maintained between 0.1 and 0.3.  Excessive distillation in low‐complexity scenarios can over‐constrain the feature space, while higher‐complexity tasks—or those with more abundant samples—benefit from stronger soft‐target regularization.  Notably, the optimal $\beta$ increases with the number of shots, suggesting that a stronger distillation signal helps suppress inter‐class confusion as the available supervision becomes richer.



In the experiments on the HUST dataset, Fig.~\ref{HUST} depicts the grid‐search surface, highlighting the model’s sensitivity to the self‐supervised weight~$\alpha$ and the distillation weight~$\beta$. In the $N$‐way 1‐shot scenario, keeping $\beta$ at a small value delivers the best and remarkably stable accuracy, indicating that an overly strong distillation signal suppresses the learning of task‐relevant features more severely than an oversized self‐supervised contribution. In contrast, the $N$‐way 5‐shot regime exhibits the opposite trend: smaller values of $\alpha$ consistently yield superior performance, while the optimal~$\beta$ shifts toward the mid–high range.  The corresponding heat map shows a pronounced “warming” in the left half of the search space, revealing that the auxiliary self‐supervised weight is effectively constrained.  As the support set enlarges, the benefit of a stronger distillation signal becomes increasingly evident, further suppressing inter‐class confusion.


In the experiments on the BJTU dataset, Fig.~\ref{BJTU} portrays the grid-search surface on the BJTU benchmark, where a trend resembling that of HUST becomes evident: the left portion of the heat map is generally “warmer,” while colour variations intensify along the vertical $\beta$ axis.  This pattern indicates that, on BJTU as well, the marginal influence of the distillation weight~$\beta$ on final accuracy is more pronounced than that of the self-supervised weight~$\alpha$.  As long as $\alpha$ is constrained to a modest range (below~0.3), the model maintains consistently high performance. In the $N$-way 1-shot setting, the impact of~$\beta$ is comparatively moderate, yielding a smoother landscape than on HUST; the optimal region again centres around $\beta=0.3$.  By contrast, the 10-way 1-shot experiment exhibits its performance peak at $\alpha=0.8$ and $\beta=0.3$, a markedly larger $\alpha$ than for the other datasets.  This observation suggests that BJTU contains greater intra-class variability, requiring the self-supervised branch to shoulder a heavier burden of representation learning to compensate for the extremely sparse supervision. In the $N$-way 5-shot regime, BJTU and HUST trace nearly identical trajectories, once again favouring “mid-to-high” $\beta$ values while keeping~$\alpha$ low.  These results corroborate our earlier findings: the distillation term~$\beta$ dictates the performance ceiling, whereas the self-supervised term~$\alpha$ primarily supplies lightweight regularisation and feature compensation.
